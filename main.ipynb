{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Building a Recurrent Neural Network\n",
    "# Sentiment Analysis\n",
    "In this project, we will build a Long Short-term memory (LSTM) neural network to solve a binary sentiment analysis problem. We will use the IMDB dataset that contains 25,000 movie reviews from IMDB users. The dataset contains an even number of positive and negative reviews. We will use 50% of the data for training, 25% for validation and 25% for testing."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "239969cb80ca3417"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import imdb\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "initial_id"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=10000, seed=42, )"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cb5e5e1df753faec"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(X_train[0])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8e3ceaff14856611"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Print the number of samples\n",
    "print(f\"X_train: {len(X_train)}\")\n",
    "print(f\"X_test: {len(X_test)}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5f20952eeda5f019"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocessing\n",
    "\n",
    "To split the dataset with 80-10-10 ratio, we will first concatenate train and datasets to create one big dataset. Then we will split the dataset into train, validation and test sets."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c698f76445b91338"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Concatenate train and test sets\n",
    "X = np.concatenate((X_train, X_test), axis=0)\n",
    "y = np.concatenate((y_train, y_test), axis=0)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8d5b5232da4e819"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Padding\n",
    "\n",
    "Since all reviews are at different lengths, we'll use padding to make all of them same length."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1d4881374d937008"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X = tf.keras.preprocessing.sequence.pad_sequences(X, maxlen=1024)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cebee6430d5cf0b1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Splitting\n",
    "\n",
    "Now, split X and y into train, validation and test dataset and assign those to corresponding values."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "71a56e993f79fbcf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create the training datasets\n",
    "X_train = X[:40000]\n",
    "y_train = y[:40000]\n",
    "# Create the validation datasets\n",
    "X_val = X[40000:45000]\n",
    "y_val = y[40000:45000]\n",
    "# Create the test datasets\n",
    "X_test = X[45000:50000]\n",
    "y_test = y[45000:50000]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3753ea7143478163"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f\"X_train: {len(X_train)}\")\n",
    "print(f\"X_val: {len(X_val)}\")\n",
    "print(f\"X_test: {len(X_test)}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d639e03f908afef9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Building the model\n",
    "\n",
    "That was it for the preprocessing of the data."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a6ac56d2b9974326"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "671217315400b7ec"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Embedding layer\n",
    "\n",
    "For the first layer, we add an embedding layer. The embedding layer converts word indexes to word vectors. Word indexes are integer representations of words and word vectors are real-valued representations of words. Word vectors are also called embeddings. We will use 128 as the embedding dimension, which means that each word will be represented by a vector of length 128."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7040dd0b1ec50bd3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.add(tf.keras.layers.Embedding(input_dim=10000, output_dim=256nput_shape=(None,)))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8decb48652242c34"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
